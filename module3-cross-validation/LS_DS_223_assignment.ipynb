{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NCkfBoSgmjX"
      },
      "source": [
        "BloomTech Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7daE7_VYgmjZ"
      },
      "source": [
        "# Module Project: Hyperparameter Tuning\n",
        "This week, the module projects will focus on creating and improving a model for the Tanazania Water Pump dataset. Your goal is to create a model to predict whether a water pump is functional, non-functional, or functional needs repair.\n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are as follows:\n",
        "\n",
        "- **Task 1:** Use `wrangle` function to import training and test data.\n",
        "- **Task 2:** Split training data into feature matrix `X` and target vector `y`.\n",
        "- **Task 3:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 4:** Build `clf_dt`.\n",
        "- **Task 5:** Build `clf_rf`.\n",
        "- **Task 6:** Evaluate classifiers using k-fold cross-validation.\n",
        "- **Task 7:** Tune hyperparameters for best performing classifier.\n",
        "- **Task 8:** Print out best score and params for model.\n",
        "- **Task 9:** Create `submission.csv` and upload to Kaggle.\n",
        "\n",
        "You should limit yourself to the following libraries for this project:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `ydata-profiling`\n",
        "- `sklearn`\n",
        "\n",
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QWbohB9rgmja"
      },
      "outputs": [],
      "source": [
        "def wrangle(fm_path, tv_path=None):\n",
        "    if tv_path:\n",
        "        df = pd.merge(pd.read_csv(fm_path,\n",
        "                                  na_values=[0, -2.000000e-08]),\n",
        "                      pd.read_csv(tv_path)).set_index('id')\n",
        "    else:\n",
        "        df = pd.read_csv(fm_path,\n",
        "                         na_values=[0, -2.000000e-08],\n",
        "                         index_col='id')\n",
        "\n",
        "    # Drop constant columns\n",
        "    df.drop(columns=['recorded_by'], inplace=True)\n",
        "\n",
        "    # Drop HCCCs\n",
        "    cutoff = 100\n",
        "    drop_cols = [col for col in df.select_dtypes('object').columns\n",
        "                 if df[col].nunique() > cutoff]\n",
        "    df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    dupe_cols = [col for col in df.head(100).T.duplicated().index\n",
        "                 if df.head(100).T.duplicated()[col]]\n",
        "    df.drop(columns=dupe_cols, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_MdfqkNgmjb"
      },
      "source": [
        "**Task 1:** Using the above `wrangle` function to read `train_features.csv` and `train_labels.csv` into the DataFrame `df`, and `test_features.csv` into the DataFrame `X_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M5nq4yYygmjc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = wrangle(\"C:/Users/ISHMO_CT/Downloads/Cross-validation/Data-sets/train_features.csv\" , \"C:/Users/ISHMO_CT/Downloads/Cross-validation/Data-sets/train_labels.csv\")\n",
        "X_test = wrangle(\"C:/Users/ISHMO_CT/Downloads/Cross-validation/Data-sets/test_features.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0uUom5tgmjd"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 2:** Split your DataFrame `df` into a feature matrix `X` and the target vector `y`. You want to predict `'status_group'`.\n",
        "\n",
        "**Note:** You won't need to do a train-test split because you'll use cross-validation instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wJSDaEeVgmjd"
      },
      "outputs": [],
      "source": [
        "target = 'status_group'\n",
        "y = df[target]\n",
        "X = df.drop(columns=target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4UdrGp7gmje"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 3:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cz6EYx6wgmje"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Accuracy Score: 0.5429828068772491\n"
          ]
        }
      ],
      "source": [
        "baseline_acc = y.value_counts(normalize=True).max()\n",
        "print('Baseline Accuracy Score:', baseline_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1clhLsPgmje"
      },
      "source": [
        "# IV. Build Models\n",
        "\n",
        "**Task 4:** Build a `Pipeline` named `clf_dt`. Your `Pipeline` should include:\n",
        "\n",
        "- an `OrdinalEncoder` transformer for categorical features.\n",
        "- a `SimpleImputer` transformer fot missing values.\n",
        "- a `DecisionTreeClassifier` Predictor.\n",
        "\n",
        "**Note:** Do not train `clf_dt`. You'll do that in a subsequent task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oJAcEyJ-gmjf"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from category_encoders import OrdinalEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "clf_dt = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    DecisionTreeClassifier()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSRNaBSjgmjf"
      },
      "source": [
        "**Task 5:** Build a `Pipeline` named `clf_rf`. Your `Pipeline` should include:\n",
        "\n",
        "- an `OrdinalEncoder` transformer for categorical features.\n",
        "- a `SimpleImputer` transformer fot missing values.\n",
        "- a `RandomForestClassifier` predictor.\n",
        "\n",
        "**Note:** Do not train `clf_rf`. You'll do that in a subsequent task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfSzu4VMgmjf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf_rf = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIaVc74zgmjf"
      },
      "source": [
        "# V. Check Metrics\n",
        "\n",
        "**Task 6:** Evaluate the performance of both of your classifiers using k-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXAc5vNUgmjf"
      },
      "outputs": [],
      "source": [
        "cv_scores_dt = ...\n",
        "cv_scores_rf = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QjPO4l5gmjg"
      },
      "outputs": [],
      "source": [
        "print('CV scores DecisionTreeClassifier')\n",
        "print(cv_scores_dt)\n",
        "print('Mean CV accuracy score:', cv_scores_dt.mean())\n",
        "print('STD CV accuracy score:', cv_scores_dt.std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSkwz_Jugmjg"
      },
      "outputs": [],
      "source": [
        "print('CV score RandomForestClassifier')\n",
        "print(cv_scores_rf)\n",
        "print('Mean CV accuracy score:', cv_scores_rf.mean())\n",
        "print('STD CV accuracy score:', cv_scores_rf.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ml7HWndgmjg"
      },
      "source": [
        "# VI. Tune Model\n",
        "\n",
        "**Task 7:** Choose the best performing of your two models and tune its hyperparameters using a `RandomizedSearchCV` named `model`. Make sure that you include cross-validation and that `n_iter` is set to at least `25`.\n",
        "\n",
        "**Note:** If you're not sure which hyperparameters to tune, check the notes from today's guided project and the `sklearn` documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKywdniegmjg"
      },
      "outputs": [],
      "source": [
        "model = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0oXkUWvgmjh"
      },
      "source": [
        "**Task 8:** Print out the best score and best params for `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w94OIgAxgmjh"
      },
      "outputs": [],
      "source": [
        "best_score = ...\n",
        "best_params = ...\n",
        "\n",
        "print('Best score for `model`:', best_score)\n",
        "print('Best params for `model`:', best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXCohrFIgmjh"
      },
      "source": [
        "# Communicate Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIXpTQZrgmjh"
      },
      "source": [
        "**Task 9:** Create a DataFrame `submission` whose index is the same as `X_test` and that has one column `'status_group'` with your predictions. Next, save this DataFrame as a CSV file and upload your submissions to our competition site.\n",
        "\n",
        "**Note:** Check the `sample_submission.csv` file on the competition website to make sure your submissions follows the same formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_fPGj5fgmjh"
      },
      "outputs": [],
      "source": [
        "submission = ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
