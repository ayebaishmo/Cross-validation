{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUhphDysw-6P"
      },
      "source": [
        "## BloomTech Data Science\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvWhcow-dvS8"
      },
      "source": [
        "# Cross-Validation\n",
        "\n",
        "- Do **k-fold cross-validation** with independent test set\n",
        "- Use scikit-learn for **hyperparameter optimization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRgkCSgGdty-"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "#!pip install category_encoders==2.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF70Jp0x4NJU"
      },
      "outputs": [],
      "source": [
        "# from category_encoders import OrdinalEncoder\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.model_selection import cross_val_score, validation_curve # k-fold CV\n",
        "# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # Hyperparameter tuning\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVzQtNC9oucF"
      },
      "source": [
        "# Downloading the Tanzania Waterpump Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-olXlnJoucR"
      },
      "source": [
        "Make sure  you only use the dataset that is available through the **DS** **Kaggle Competition**. DO NOT USE any other Tanzania waterpump datasets that you might find online.\n",
        "\n",
        "There are two ways you can get the dataset. Make sure you have joined the competition first!:\n",
        "\n",
        "1. You can download the dataset directly by accessing the challenge and the files through the Kaggle Competition URL on Canvas (make sure you have joined the competition!)\n",
        "\n",
        "2. Use the Kaggle API using the code in the following cells. This article provides helpful information on how to fetch your Kaggle Dataset into Google Colab using the Kaggle API.\n",
        "\n",
        "> https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6TZ5nDFYkCa"
      },
      "source": [
        "# Using Kaggle API to download datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2e6fPUATxLZ",
        "outputId": "b8cb2a8b-707c-4da8-e5b6-ff9b88f48493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# # mounting your google drive on colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYSpUv9uYBAo",
        "outputId": "2e6a88bf-6b0b-4a8e-aeac-e0919d4a78fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ]
        }
      ],
      "source": [
        "#change your working directory, if you want to or have already saved your kaggle dataset on google drive.\n",
        "# %cd /content/gdrive/My Drive/Kaggle\n",
        "# update it to your folder location on drive that contians the dataset and/or kaggle API token json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXChvgdZYb_t"
      },
      "outputs": [],
      "source": [
        "# Download your Kaggle Dataset, if you haven't already done so.\n",
        "# import os\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "# !kaggle competitions download -c bloomtech-water-pump-challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB84qgRRYdDF"
      },
      "outputs": [],
      "source": [
        "# Unzip your Kaggle dataset, if you haven't already done so.\n",
        "# !unzip \\*.zip  && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eag2zYiQYf6q"
      },
      "outputs": [],
      "source": [
        "# List all files in your Kaggle folder on your google drive.\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCBYPw7kd1AN"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jaXMenXoZ0D6"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train_features.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m   df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mdupe_cols, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 34\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mwrangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfm_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_features.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_labels.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m X_test \u001b[38;5;241m=\u001b[39m wrangle(fm_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mwrangle\u001b[1;34m(fm_path, tv_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrangle\u001b[39m(fm_path, tv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tv_path:\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfm_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2.000000e-08\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_recorded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      7\u001b[0m                   pd\u001b[38;5;241m.\u001b[39mread_csv(tv_path))\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(fm_path,\n\u001b[0;32m     10\u001b[0m                      na_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.000000e-08\u001b[39m],\n\u001b[0;32m     11\u001b[0m                      parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_recorded\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m                      index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ISHMO_CT\\.conda\\envs\\Kaggle_proj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ISHMO_CT\\.conda\\envs\\Kaggle_proj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\ISHMO_CT\\.conda\\envs\\Kaggle_proj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ISHMO_CT\\.conda\\envs\\Kaggle_proj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\ISHMO_CT\\.conda\\envs\\Kaggle_proj\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_features.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "def wrangle(fm_path, tv_path=None):\n",
        "  if tv_path:\n",
        "    df = pd.merge(pd.read_csv(fm_path,\n",
        "                              na_values=[0, -2.000000e-08],\n",
        "                              parse_dates=['date_recorded']),\n",
        "                  pd.read_csv(tv_path)).set_index('id')\n",
        "  else:\n",
        "    df = pd.read_csv(fm_path,\n",
        "                     na_values=[0, -2.000000e-08],\n",
        "                     parse_dates=['date_recorded'],\n",
        "                     index_col='id')\n",
        "\n",
        "  # Drop constant columns\n",
        "  df.drop(columns=['recorded_by'], inplace=True)\n",
        "\n",
        "  # Create age feature\n",
        "  df['pump_age'] = df['date_recorded'].dt.year - df['construction_year']\n",
        "  df.drop(columns='date_recorded', inplace=True)\n",
        "\n",
        "  # Drop HCCCs\n",
        "  cutoff = 100\n",
        "  drop_cols = [col for col in df.select_dtypes('object').columns\n",
        "              if df[col].nunique() > cutoff]\n",
        "  df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "  # Drop duplicate columns\n",
        "  dupe_cols = [col for col in df.head(100).T.duplicated().index # change 15 to 100!!!!\n",
        "               if df.head(100).T.duplicated()[col]]\n",
        "  df.drop(columns=dupe_cols, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "df = wrangle(fm_path='train_features.csv',\n",
        "             tv_path='train_labels.csv')\n",
        "\n",
        "X_test = wrangle(fm_path='test_features.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wupyZysm_avz"
      },
      "outputs": [],
      "source": [
        "df.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jetWccxMqmzY"
      },
      "source": [
        "# II. Split Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-MPA0qlr-mK"
      },
      "source": [
        "## Split TV from FM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iN01DozUbyH"
      },
      "outputs": [],
      "source": [
        "target = 'status_group'\n",
        "y = df[target]\n",
        "X = df.drop(columns = target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8jSWomGsLsw"
      },
      "source": [
        "# Training-Validation Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX9uvMMgs6J_"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "This is a **classification** problem, our baseline will be **accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkUJVqSuULDg"
      },
      "outputs": [],
      "source": [
        "y.value_counts(normalize = True).max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA19NsrFtgTT"
      },
      "source": [
        "# IV. Build Models\n",
        "\n",
        "- `DecisionTreeClassifier`\n",
        "- `RandomForestClassifier`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_ZpJSwObNl_"
      },
      "outputs": [],
      "source": [
        "model_dt = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    DecisionTreeClassifier(random_state=42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4fO6aXEbNiQ"
      },
      "outputs": [],
      "source": [
        "model_rf = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(n_estimators =25, random_state=42)\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l96Atv4REyH3"
      },
      "source": [
        "**Check cross-validation scores**\n",
        "\n",
        "![Cross Validation](https://upload.wikimedia.org/wikipedia/commons/4/4b/KfoldCV.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLx-EHNGB-xq"
      },
      "outputs": [],
      "source": [
        "print('CV score DecisionTreeClassifier')\n",
        "print('Mean CV accuracy score:', )\n",
        "print('STD CV accuracy score:',)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLIdbPJpCC_S"
      },
      "outputs": [],
      "source": [
        "print('CV score RandomForestClassifier')\n",
        "print('Mean CV accuracy score:',)\n",
        "print('STD CV accuracy score:',)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSTF3aOWo-5-"
      },
      "source": [
        "# V. Tune Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYCO2Cj4ILg"
      },
      "source": [
        "Different hyperparameter values I want to try out :\n",
        "\n",
        "*   Simpleimputer - mean, median - 2 values\n",
        "*   max_depth - range(5,40,5) - 7 values\n",
        "*   n_estimators - range(25,125,25) - 4 values\n",
        "\n",
        "> Total combinations of these hyperparameters = 2 * 7 * 4 = 56\n",
        "\n",
        "Testing out the above hyperparameter combinations with 5-fold Cross Validation will need :\n",
        "\n",
        "> Total number of models to be fit = 2 * 7 * 4 * 5 = 280\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgK-doeJWMAE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrMYSoldWL7l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRL-I2ensgom"
      },
      "source": [
        "**`GridSearch`:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f27tFQRMWARw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPvNOV6Cw1Zl"
      },
      "source": [
        "**`RandomizedSearchCV`:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFXI-drCV9LB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT5O4MpOV4qj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P_b_2u1V9II"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPydrAciFvW"
      },
      "source": [
        "# VI. Communicate Results\n",
        "\n",
        "**Showing Feature Importance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orrZCcWqj0Wu"
      },
      "source": [
        "Plot the feature importance for our `RandomForest` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3QZfcikWWW_"
      },
      "outputs": [],
      "source": [
        "bestestimator =\n",
        "importances = bestestimator.named_steps['randomforestclassifier'].feature_importances_\n",
        "features = X.columns\n",
        "feat_imp = pd.Series(importances, index=features).sort_values()\n",
        "feat_imp.tail(10).plot(kind='barh')\n",
        "plt.xlabel('Reduction in Gini Impurity');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIiBxuSrDPZU"
      },
      "source": [
        "# VII. Make Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbEIotkT8eUM"
      },
      "outputs": [],
      "source": [
        "y_pred = model_rfrs.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFL1Zdi88f98"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({'status_group':y_pred}, index=X_test.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNjXGaXEM9Po"
      },
      "outputs": [],
      "source": [
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvrFM81t9Fqv"
      },
      "outputs": [],
      "source": [
        "pd.Timestamp.now().strftime('%Y-%m-%d_%H%M_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sDIlJ1fDPZW"
      },
      "outputs": [],
      "source": [
        "datestamp = pd.Timestamp.now().strftime('%Y-%m-%d_%H%M_') #string from time format\n",
        "submission.to_csv(f'{datestamp}submission.csv') #format string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS6EDRbCbr-F"
      },
      "source": [
        "# VIII. Saving a trained model to reuse it in the future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1H0NSTQUh-A"
      },
      "outputs": [],
      "source": [
        "# Once you have found the best model, you might as well save it and then reload it when you want to test it later\n",
        "\n",
        "# save model\n",
        "import pickle\n",
        "\n",
        "filename =\n",
        "\n",
        "#save your model (it will be stored in your current working directory - download to your computer if GDrive is not mounted)\n",
        "pickle.dump(model_rf,open(filename,'wb'))\n",
        "#load model\n",
        "model_rf_loaded = pickle.load(open(filename,'rb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
